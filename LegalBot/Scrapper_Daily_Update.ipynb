{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Required Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import fitz\n",
    "import tempfile\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "current_year = time.gmtime().tm_year\n",
    "current_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Important HREF Links for the Daily Updates Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL_DAILY_UPDATE = {\n",
    "#     'UK' : 'https://www.legislation.gov.uk/new/uk',\n",
    "#     'Wales' : 'https://www.legislation.gov.uk/new/wales',\n",
    "#     'Scotland' : 'https://www.legislation.gov.uk/new/scotland',\n",
    "#     'Northern Ireland' : 'https://www.legislation.gov.uk/new/ni',\n",
    "# }\n",
    "\n",
    "URL_DAILY_UPDATE = {\n",
    "    'Northern Ireland' : 'https://www.legislation.gov.uk/new/ni',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definign Some Functions Which Will be Used for The Daily Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_daily_update(driver):\n",
    "    '''A function that chechs if a new published daily update exists in a tab'''\n",
    "    h5_content = driver.find_element(By.CLASS_NAME, 'p_content').find_element(By.TAG_NAME, 'h5').text\n",
    "    if h5_content == 'Nothing published on this date':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_content_from_pdf(pdf_url):\n",
    "    response = requests.get(f'{pdf_url}')\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_pdf_file:\n",
    "        temp_pdf_path = temp_pdf_file.name\n",
    "        response = requests.get(pdf_url)\n",
    "        temp_pdf_file.write(response.content)\n",
    "\n",
    "    # Open the temporary PDF file and extract text content\n",
    "    pdf_document = fitz.open(temp_pdf_path)\n",
    "    text_content = ''\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text_content += page.get_text()\n",
    "    pdf_document.close()\n",
    "\n",
    "    return text_content\n",
    "\n",
    "def extract_content(driver, title_url):\n",
    "    '''Function that extracts the content from a tab. NOte: This is same as in Scrapper.ipynb'''\n",
    "    driver.get(title_url)\n",
    "    time.sleep(1)\n",
    "    Title_Content_Div = driver.find_element(By.CSS_SELECTOR, 'div.legToc')\n",
    "    NavBar = Title_Content_Div.find_element(By.ID, 'legSubNav')\n",
    "    NavBarLists = NavBar.find_elements(By.TAG_NAME, 'li')\n",
    "    ContentTab = NavBarLists[1] #It may be clickable or not. If not, the media type is PDF not text\n",
    "    Content_Link_Tag = None\n",
    "    try:  #If Not PDF\n",
    "        Content_Link_Tag = ContentTab.find_element(By.TAG_NAME, 'a')\n",
    "    except:\n",
    "        Content_Link_Tag = None\n",
    "    \n",
    "    if Content_Link_Tag != None:\n",
    "        Content_Link_Tag_Href = Content_Link_Tag.get_attribute('href')\n",
    "        driver.get(Content_Link_Tag_Href)\n",
    "        time.sleep(1)\n",
    "        '''Now get the content'''\n",
    "        '''Multiple Pages of the content page'''\n",
    "        Page_Number = 1\n",
    "        All_Provisions_Text = ''\n",
    "        while True:\n",
    "            Content_Box = driver.find_element(By.ID, 'content')\n",
    "            Content_Text = Content_Box.find_element(By.ID, 'viewLegContents').find_element(By.CLASS_NAME, 'LegSnippet')\n",
    "            page_Text = Content_Text.text\n",
    "            All_Provisions_Text += page_Text\n",
    "            print(f'Page Number: {Page_Number}')\n",
    "            \n",
    "            '''Now check for button'''\n",
    "            Button_Panel = driver.find_element(By.CLASS_NAME, 'prevNextNav')\n",
    "            try:\n",
    "                Next_Button = Button_Panel.find_element(By.TAG_NAME, 'ul').find_elements(By.TAG_NAME, 'li')[-1].find_element(By.TAG_NAME, 'a')\n",
    "                print(f'Next Button found: {Next_Button.text}')\n",
    "                try:\n",
    "                    Next_Button.click()\n",
    "                    time.sleep(1)\n",
    "                    Page_Number += 1\n",
    "                except:\n",
    "                    print(f'You are probably on the very last Provision page')\n",
    "                    print(f'Provision Page Number: {Page_Number}')\n",
    "                    break\n",
    "            except:\n",
    "                print(f'No Next Button Found - Last Provision Page')\n",
    "                print(f'Provision Page Number: {Page_Number}')\n",
    "                break\n",
    "        return All_Provisions_Text\n",
    "    \n",
    "    elif Content_Link_Tag == None:\n",
    "        Tag_PDF_href = driver.find_element(By.CSS_SELECTOR, 'div.LegSnippet').find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        pdf_content = extract_content_from_pdf(Tag_PDF_href)\n",
    "        return pdf_content\n",
    "\n",
    "def get_daily_update(driver, url):\n",
    "    '''A function that extracts the daily update from a tab'''\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if verify_daily_update(driver) == True:\n",
    "        New_Titles = {}\n",
    "        Titles_Href_List = []\n",
    "        Title_Name_List = []\n",
    "\n",
    "        Content_div = driver.find_element(By.CLASS_NAME, 'p_content')\n",
    "        Legislation_Name = Content_div.find_element(By.TAG_NAME, 'h5').text\n",
    "        Title_URLS = Content_div.find_elements(By.TAG_NAME, 'h6')\n",
    "\n",
    "        for idxNewTitle, title in enumerate(Title_URLS):\n",
    "            href = title.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "            name = title.text.split('-')[-1].strip()\n",
    "            Titles_Href_List.append(href)\n",
    "            Title_Name_List.append(name)\n",
    "\n",
    "        if len(Titles_Href_List) > 0:\n",
    "            New_Titles[f'{Legislation_Name}'] = dict(zip(Title_Name_List, Titles_Href_List))\n",
    "            return New_Titles\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "def create_dirs(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Country_Key in URL_DAILY_UPDATE.keys():\n",
    "    New_Titles = get_daily_update(driver, URL_DAILY_UPDATE[Country_Key])\n",
    "    if New_Titles != {} and New_Titles is not None:\n",
    "        for idxLegislation, legislation_name in enumerate(New_Titles.keys()):\n",
    "            for title_name, title_url in New_Titles[legislation_name].items():\n",
    "                '''Now since we have the title url, we can extract the title data'''\n",
    "                print(f'{Country_Key} - {legislation_name} - {title_name} - {title_url}')\n",
    "                \n",
    "                Title_Data_Content = extract_content(driver, title_url)\n",
    "\n",
    "                create_dirs(path=f'./New_Content/{Country_Key}/{legislation_name}/{current_year}')\n",
    "                with open(f'./New_Content/{Country_Key}/{legislation_name}/{current_year}/{title_name}.txt', 'w') as f:\n",
    "                    f.write(Title_Data_Content)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
