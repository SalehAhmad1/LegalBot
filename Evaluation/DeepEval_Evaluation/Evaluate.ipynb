{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../LegalBot/Database'+'/.env')\n",
    "\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "from deepeval.metrics import ContextualPrecisionMetric, ContextualRecallMetric\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from LegalBot.RAG_v1 import RAG_Bot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_metric = FaithfulnessMetric(threshold=0.5)\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "context_precision_metric = ContextualPrecisionMetric(threshold=0.5)\n",
    "context_recall_metric = ContextualRecallMetric(threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions for Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_answer_relevancy(query, actual_answer, LLM_Response, retrieved_contexts):\n",
    "    test_case = LLMTestCase(\n",
    "        input=f'{query},',\n",
    "        actual_output=f'{LLM_Response}',\n",
    "        retrieval_context=retrieved_contexts,\n",
    "    )\n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    answer_relavancy_score = answer_relevancy_metric.score\n",
    "    answer_relavancy_reason = answer_relevancy_metric.reason\n",
    "    return answer_relavancy_score, answer_relavancy_reason\n",
    "\n",
    "def test_faithfulness(query, actual_answer, LLM_Response, retrieved_contexts):\n",
    "    test_case = LLMTestCase(\n",
    "        input=f'{query},',\n",
    "        actual_output=f'{LLM_Response}',\n",
    "        retrieval_context=retrieved_contexts,\n",
    "    )\n",
    "    faithfulness_metric.measure(test_case)\n",
    "    faithfulness_score = faithfulness_metric.score\n",
    "    faithfulness_reason = faithfulness_metric.reason\n",
    "    return faithfulness_score, faithfulness_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_context_precision(query, actual_answer, LLM_Response, retrieved_contexts):\n",
    "    test_case = LLMTestCase(\n",
    "        input=f'{query},',\n",
    "        actual_output=f'{LLM_Response}',\n",
    "        expected_output=f'{actual_answer}',\n",
    "        retrieval_context=retrieved_contexts,\n",
    "    )\n",
    "    context_precision_metric.measure(test_case)\n",
    "    context_precision_score = context_precision_metric.score\n",
    "    context_precision_reason = context_precision_metric.reason\n",
    "    return context_precision_score, context_precision_reason\n",
    "\n",
    "def test_context_recall(query, actual_answer, LLM_Response, retrieved_contexts):\n",
    "    test_case = LLMTestCase(\n",
    "        input=f'{query},',\n",
    "        actual_output=f'{LLM_Response}',\n",
    "        expected_output=f'{actual_answer}',\n",
    "        retrieval_context=retrieved_contexts,\n",
    "    )\n",
    "    context_recall_metric.measure(test_case)\n",
    "    context_recall_score = context_recall_metric.score\n",
    "    context_recall_reason = context_recall_metric.reason\n",
    "    return context_recall_score, context_recall_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an Instance of the RAG chatbot to get responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_names = ['Uk', 'Wales', 'NothernIreland', 'Scotland']\n",
    "bot = RAG_Bot(collection_names=collection_names, text_splitter='SpaCy', embedding_model=\"SentenceTransformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the benchmark data on a combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_grid = {\n",
    "#     'k' : [5,7,9,11,13,15,17,19,21,23,25],\n",
    "#     'search_type' : ['Hybrid', 'Vector'],\n",
    "#     'multi_query' : [True, False],\n",
    "#     'rerank' : [True, False],\n",
    "# }\n",
    "\n",
    "# for idx_k, k in enumerate(params_grid['k']):\n",
    "#     for idx_search_type, search_type in enumerate(params_grid['search_type']):\n",
    "#         for idx_multi_query, multi_query in enumerate(params_grid['multi_query']):\n",
    "#             for idx_rerank, rerank in enumerate(params_grid['rerank']):\n",
    "#                 file_name = f'./Responses/RAG_Bot_Responses search type {search_type} rerank {rerank} multi query {multi_query} k {k}.csv'\n",
    "#                 os.makedirs('./Responses', exist_ok=True)\n",
    "#                 benchmark_data_df = pd.read_csv('../Evaluation_Dataset/Benchmark-Data.csv')\n",
    "\n",
    "#                 print(f'Current Parameters Grid: search_type: {search_type}, rerank: {rerank}, multi_query: {multi_query}, k: {k}')\n",
    "\n",
    "#                 results = []\n",
    "#                 for idx_row, row in benchmark_data_df.iterrows():\n",
    "#                     country = row['Country']\n",
    "#                     prompt = row['Question']\n",
    "#                     actual_answer = row['Actual Answer']\n",
    "\n",
    "#                     (response, individual_context_texts) = bot.query(\n",
    "#                         query = country+prompt,\n",
    "#                         k = k,\n",
    "#                         search_type = search_type,\n",
    "#                         multi_query = multi_query,\n",
    "#                         rerank = rerank,\n",
    "#                         verbose = False,\n",
    "#                         mode = 'eval'\n",
    "#                     )\n",
    "\n",
    "#                     # Create a dictionary to store the results of this iteration\n",
    "#                     result = {\n",
    "#                         'Country': country,\n",
    "#                         'Prompt': prompt,\n",
    "#                         'Actual Answer': actual_answer,\n",
    "#                         'Response': response,\n",
    "#                         'k': k,\n",
    "#                         'search_type': search_type,\n",
    "#                         'multi_query': multi_query,\n",
    "#                         'rerank': rerank\n",
    "#                     }\n",
    "\n",
    "#                     # Add individual context texts as separate columns\n",
    "#                     for idx_context, context in enumerate(individual_context_texts):\n",
    "#                         result[f'Context_{idx_context+1}'] = context\n",
    "#                     results.append(result)\n",
    "\n",
    "#                 results_df = pd.DataFrame(results)\n",
    "#                 results_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relavancy && Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_csv, csv in enumerate(os.listdir('./Responses')):\n",
    "    Answer_Relavancy_Scores = []\n",
    "    Answer_Relavancy_Reasons = []\n",
    "    Faithfulness_Scores = []\n",
    "    Faithfulness_Reasons = []\n",
    "\n",
    "    Context_Precision_Scores = []\n",
    "    Context_Precision_Reasons = []\n",
    "    Context_Recall_Scores = []\n",
    "    Context_Recall_Reasons = []\n",
    "\n",
    "    Scores_DF_Path = os.path.join('./Scores', f'{csv}')\n",
    "\n",
    "    data = pd.read_csv(f'./Responses/{csv}')\n",
    "    for idx, row in data.iterrows():\n",
    "        row = row.dropna()\n",
    "        selected_cols = row.filter(like='Context_')\n",
    "        score, reason = test_answer_relevancy(row['Prompt'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "        Answer_Relavancy_Scores.append(score)\n",
    "        Answer_Relavancy_Reasons.append(reason)\n",
    "\n",
    "        score, reason = test_faithfulness(row['Prompt'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "        Faithfulness_Scores.append(score)\n",
    "        Faithfulness_Reasons.append(reason)\n",
    "\n",
    "        score, reason = test_context_precision(row['Prompt'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "        Context_Precision_Scores.append(score)\n",
    "        Context_Precision_Reasons.append(reason)\n",
    "\n",
    "        score, reason = test_context_recall(row['Prompt'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "        Context_Recall_Scores.append(score)\n",
    "        Context_Recall_Reasons.append(reason)\n",
    "        \n",
    "    Score_DF = pd.DataFrame({\n",
    "        'Answer Relavancy Score': Answer_Relavancy_Scores,\n",
    "        'Answer Relavancy Reason': Answer_Relavancy_Reasons,\n",
    "        'Faithfulness Score': Faithfulness_Scores,\n",
    "        'Faithfulness Reason': Faithfulness_Reasons,\n",
    "        'Context Precision Score': Context_Precision_Scores,\n",
    "        'Context Precision Reason': Context_Precision_Reasons,\n",
    "        'Context Recall Score': Context_Recall_Scores,\n",
    "        'Context Recall Reason': Context_Recall_Reasons\n",
    "    })\n",
    "\n",
    "    Score_DF.to_csv(Scores_DF_Path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer_Relavancy_Scores = []\n",
    "# Answer_Relavancy_Reasons = []\n",
    "# Faithfulness_Scores = []\n",
    "# Faithfulness_Reasons = []\n",
    "# for idx, row in data.iterrows():\n",
    "#     selected_cols = row[['Response1', 'Response2', 'Response3']]\n",
    "#     score, reason = test_answer_relevancy(row['Question'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "#     Answer_Relavancy_Scores.append(score)\n",
    "#     Answer_Relavancy_Reasons.append(reason)\n",
    "    \n",
    "#     score, reason = test_faithfulness(row['Question'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "#     Faithfulness_Scores.append(score)\n",
    "#     Faithfulness_Reasons.append(reason)\n",
    "    \n",
    "# Generation_Eval_DF = pd.DataFrame({\n",
    "#     'Answer Relavancy Score': Answer_Relavancy_Scores,\n",
    "#     'Answer Relavancy Reason': Answer_Relavancy_Reasons,\n",
    "#     'Faithfulness Score': Faithfulness_Scores,\n",
    "#     'Faithfulness Reason': Faithfulness_Reasons\n",
    "# })\n",
    "# Generation_Eval_DF.to_csv('Generation_Metric_Evaluation.csv', index=False)\n",
    "# Generation_Eval_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context_Precision_Scores = []\n",
    "# Context_Precision_Reasons = []\n",
    "# Context_Recall_Scores = []\n",
    "# Context_Recall_Reasons = []\n",
    "# for idx, row in data.iterrows():\n",
    "#     selected_cols = row[['Response1', 'Response2', 'Response3']]\n",
    "#     score, reason = test_context_precision(row['Question'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "#     Context_Precision_Scores.append(score)\n",
    "#     Context_Precision_Reasons.append(reason)\n",
    "    \n",
    "#     score, reason = test_context_recall(row['Question'], row['Actual Answer'], row['Response'], selected_cols.values.tolist())\n",
    "#     Context_Recall_Scores.append(score)\n",
    "#     Context_Recall_Reasons.append(reason)\n",
    "    \n",
    "# Retrieval_Eval_DF = pd.DataFrame({\n",
    "#     'Context Precision Score': Context_Precision_Scores,\n",
    "#     'Context Precision Reason': Context_Precision_Reasons,\n",
    "#     'Context Recall Score': Context_Recall_Scores,\n",
    "#     'Context Recall Reason': Context_Recall_Reasons\n",
    "# })\n",
    "# Retrieval_Eval_DF.to_csv('Retrieval_Metric_Evaluation.csv', index=False)\n",
    "# Retrieval_Eval_DF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
